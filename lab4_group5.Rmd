---
title: "Lab 4 - Cloud Data, Stat 215A, Fall 2022"
author: "Sizhu Lu, Kaitlin Smith, Ghafar Yerima"
date: "`r format(Sys.time(), '%B %d, %Y')`"
header-includes:
   - \usepackage{float}
output: 
  pdf_document:
    number_sections: true
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
# load in useful packages
library(tidyverse)
library(stringr)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(corrplot)
library(purrr)
library(gridExtra)
library(knitr)
library("GGally")
library(png)
library(grid)

# set default knitr chunks
knitr::opts_chunk$set(
  echo = FALSE,  # don't print the code chunk
  warning = FALSE,  # don't print warnings
  message = FALSE,  # don't print messages
  fig.width = 6,  # set default width of figures
  fig.height = 4,  # set default height of figures
  fig.align = "center",  # always align figure in center
  fig.pos = "H",  # always plot figure at the exact location of the code chunk
  cache = FALSE)  # don't cache results

```

```{r load-data, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
feature_names <- c("x", "y", "label", "NDAI", "SD","CORR", "DF", "CF", "BF", "AF", "AN")
img1 <- read.csv("data/img1_features.csv")
img1$label_fact <- as.factor(img1$label) # make label a factor for plotting, but needs to be a number for correlation
img1 <- img1[-c(16)] # remove extra feature I accidentally only generated for this one image
img2 <- read.csv("data/img2_features.csv")
img2$label_fact <- as.factor(img2$label)
img3 <- read.csv("data/img3_features.csv")
img3$label_fact <- as.factor(img3$label)

img1$label_final <- replace(img1$label, img1$label == 0, 1)
img2$label_final <- replace(img2$label, img2$label == 0, 1)
img3$label_final <- replace(img3$label, img3$label == 0, 1)

```

# Introduction

  Global warming is characterized by the rapid increase in Earth's surface temperature over the past century; It is due primarily to human activities including fossil fuels burning, which increases greenhouse gases (Rexford 2020). The Earth surface temperature has increased by nearly 1 degree Celsius between 1905 and 2005, and is predicted to increase between 2-6 degree Celsius by the end of the 21st century (Rexford 2020). Models have predicted that the strongest dependencies between greenhouse gases level and earth's surface temperature will occur at the Arctic (Shi et al. 2008). As the Arctic warms faster the rest of the globe, alterations in the properties and distribution of ice-and snow-covered surfaces, atmospheric vapor, and clouds can lead to further warming, and more susceptibility to greenhouse gases levels (Shi et al. 2008). To understand these intricate relationships, a study of the cloud coverage at the Arctic is necessary as clouds play a crucial role in tuning the sensitivity of the Arctic to the rise of surface air temperatures (Shi et al. 2008).\  
  
  However, studying Arctic clouds is challenging because liquid and ice water cloud particles have similar physical properties to ice and snow surfaces particles. Thus, distinguishing Arctic clouds from water or ice using their electromagnetic radiation is challenging. Thanks to a National Aeronautics, and Space Administration (NASA) effort, a Multiangle Imaging SpectroRadiometer (MISR) was launched on the Terra satellite in 1999. This radiometer produces novel radiation measurements from nine view angles. The nine view zenith angles of the cameras are 70.5" (Df), 60.0" (Cf), 45.6" (Bf), and 26.1" (Af) in the forward direction; 0.0" (An) in the nadir direction and 26.1" (Aa), 45.6" (Ba), 60.0" (Ca), and 70.5" (Da) in the aft direction. Thus, the MISR produces a large amount of data thanks to its 360 km-wide swath coverage on the Earth's surface that extends from the Arctic down to Antarctica in approximately 45 minutes.\  

  Although the MISR produces a high resolution data, its operational cloud detection system was designed before its launch and proved to not be effective for detecting clouds over bright surfaces in polar regions. Solving the polar cloud detection problem requires efficient statistical models, capable of handling the massive data set while incorporating scientific and operational considerations.\  
  
  In this paper, we explored a MISR data of polar regions and implemented various polar cloud detection algorithms based on expert labels. The algorithms implemented include logistic regression (w/ & w/o regularization), KNN, RF, QDA, and SVM. Ultimately, we aimed to design algorithms that will perform well on data sets lacking any expert labels. 
  

# Data

  Our data comprises 3 images which represent pictures taken from the MISR. Each image text file comprises 11 columns. The first two columns represent the y and x coordinates of the pixel, and the third column represent the expert label (+1= cloud, -1= not cloud, 0= unlabeled). The fourth, fifth, and sixth columns were features engineered by Shi et. al. They are: the Normalized Difference Angular Index (NDAI) which characterizes the variations in a scene with the variations in the MISR view direction, the standard deviation (SD) of MISR nadir camera pixel values across a scene, and the correlation (CORR) of MISR images of the same scene from different MISR viewing directions (Shi et al. 2008). Columns 7 to 11 represent the radiance angles DF, CF, BF, AF, and AN from the MISR.
  
## Data Collection

  The data used in our study is a subset of the MISR data set from Shi et. al. It was collected from 10 MISR orbits of path 26 over the Arctic, Northern Greenland, and Baffin Bay (Shi et al. 2008). As the MISR collects data from 233 geographically distinct but overlapping paths on a repeat cycle over 16 days, the 10 orbits spanned approximately 144 days from April 28 to September 19, 2002. Path 26 was chosen for the richness of its surface features and their relative stability over the study period. From the 6 data units per orbits included in their study, 3 were excluded as they were open water after the ice melted over the summer.  

## Data Exploration

### 1. Expert labels 

First, we'll look at the the expert labels for the presece of clouds:
<br> 
<br> 

```{r expert labels, fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="Expert Cloud Labels" }
heatmap_ggplot <- function(index, img){
  return(ggplot(img, aes(x = x,y = y, fill = img[,index])) + geom_tile() + labs(title = colnames(img)[index]) + theme(panel.background = element_blank(),plot.background = element_blank(), axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()))
}

gt_heatmap <- function(index, img){
  return(ggplot(img, aes(x = x,y = y, fill = label_fact)) + geom_tile() + labs(title = paste("Ground Truth for Image ", index)) +
  scale_fill_brewer(labels = c("Not Cloud", "Unlabeled", "Cloud"), palette = "Blues", direction = -1, name = "") + theme(panel.background =     element_blank(),plot.background = element_blank(), axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()))
}

map1 <- gt_heatmap(1, img1)
map2 <- gt_heatmap(2, img2)
map3 <- gt_heatmap(3, img3)

ggarrange(plotlist = c(list(map1), list(map2), list(map3)), ncol = 3, common.legend = TRUE)
``` 

  Looking at the initial expert labels for image 1 and 3, we notice that the clouds occupy roughly the third of the map. However, the proportion of labeled clouds on image 2 is substantially larger. The different radiance angles seem to be correlated for the most part. They show approximately the same distribution and intensity of pixels. 
  
### Rational for Converting Unlabeled pixels to "Cloud" Labels 

  Next, we explored the distribution of the cloud labels. It appeared that the cloud labels is contained in the labels that were unlabeled. For this reason, we decided to consider the unlabeled pixels as cloudy pixels.

```{r NDAI distribution, fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="Plotting distributions of cloud expert labels vs NDAI" }
# TODO: CHANGE COLORS

ndai_label1 <- ggplot(img1, aes(x = NDAI, fill = factor(label))) + geom_density(alpha = 0.4)+ labs(title = "Image1") + scale_fill_manual(labels=c("Not Cloud", "Unlabeled", "Cloud"), values = c("blue", "red", "green"), name = "") + theme_bw()

ndai_label2 <- ggplot(img2, aes(x = NDAI, fill = factor(label))) + geom_density(alpha = 0.4) + labs(title = "Image2") + scale_fill_manual(labels=c("Not Cloud", "Unlabeled", "Cloud"), values = c("blue", "red", "green"), name = "")+ theme_bw()

ndai_label3 <- ggplot(img3, aes(x = NDAI, fill = factor(label))) + geom_density(alpha = 0.4) + labs(title = "Image3") +scale_fill_manual(labels=c("Not Cloud", "Unlabeled", "Cloud"), values = c("blue", "red", "green"), name = "")+ theme_bw()

ggarrange(ndai_label1, ndai_label2, ndai_label3, ncol = 3, common.legend = TRUE)
```

Thus, we added "label_final" feature that converts all of the unlabeled pixels to cloud labels. Following this conversion, we notice that our images now contain more cloudy pixels than non-cloudy ones.

```{r}
img1$label_final <- replace(img1$label, img1$label == 0, 1)
img2$label_final <- replace(img2$label, img2$label == 0, 1)
img3$label_final <- replace(img3$label, img3$label == 0, 1)

```

### 2. 

First, we'll look at relationships between the radiances of different angles visually.

```{r image1, fig.width = 15, fig.height=10, fig.asp= 0.6, out.width='100%', out.height='100%', fig.cap="Plotting angles on maps" }
heatmap_ggplot <- function(index, img){
  return(ggplot(img, aes(x = x,y = y, fill = img[,index])) + geom_tile() + labs(title = colnames(img)[index]) + theme(panel.background =     element_blank(),plot.background = element_blank()))
}

ang1 <- lapply(7:11, heatmap_ggplot, img1)
ang2 <- lapply(7:11, heatmap_ggplot, img2)
ang3 <- lapply(7:11, heatmap_ggplot, img3)

angle1 <- ggarrange(plotlist = ang1, common.legend = TRUE, ncol = 5)
angle1 <- annotate_figure(angle1, top = text_grob("Angles for Image 1"))

angle2 <- ggarrange(plotlist = ang2, common.legend = TRUE, ncol = 5)
angle2 <- annotate_figure(angle2, top = text_grob("Angles for Image 2"))

angle3 <- ggarrange(plotlist = ang3, common.legend = TRUE, ncol = 5)
angle3 <- annotate_figure(angle3, top = text_grob("Angles for Image 3"))

ggarrange(plotlist = c(list(angle1), list(angle2), list(angle3)), nrow = 3)
``` 

Visually, it appears like the DF and AN bands are inverses of one another. Particularly
 for Image 2, When the DF band registers higher values, the AN band registers lower quantities. Comparing these plots to the expert labels, it appears like clouds 
 correspond to high values of DF and CF, which is particularly apparent in image 2.
 
 Now, looking at the CORR, NDAI, and SD features, we see stronger relationships
 between the presence of clouds and these features: 
 
```{r features, fig.width = 15, fig.height=10, fig.asp= 0.6, out.width='100%', out.height='100%', fig.cap="Plotting features on maps", fig.align="center"}
ang1 <- lapply(4:6, heatmap_ggplot, img1)
ang2 <- lapply(4:6, heatmap_ggplot, img2)
ang3 <- lapply(4:6, heatmap_ggplot, img3)

angle1 <- ggarrange(plotlist = ang1, common.legend = TRUE, ncol = 5)
angle1 <- annotate_figure(angle1, top = text_grob("Features for Image 1"))

angle2 <- ggarrange(plotlist = ang2, common.legend = TRUE, ncol = 5)
angle2 <- annotate_figure(angle2, top = text_grob("Features for Image 2"))

angle3 <- ggarrange(plotlist = ang3, common.legend = TRUE, ncol = 5)
angle3 <- annotate_figure(angle3, top = text_grob("Features for Image 3"))

ggarrange(plotlist = c(list(angle1), list(angle2), list(angle3)), nrow = 3)

```
 
It appears as though high values of NDAI indicate the presence of clouds. However,
the highest values of NDAI correspond to the regions that the expert failed to label.
More subtlety, you can see that higher values of CORR also indicate the presence of 
clouds. 

  Our next step was exploring the features quantitatively. Thus, we investigated the correlation of our features to the labels for each image. As shown below, it appears that NDAI has the strongest correlation with the labels. For image1 and image2, it appears that the angles of radiance also have a negative correlation. However, image3 does not have this relationship with the labels. In addition, it appears that NDAI and SD have a positive correlation.
  
  
```{r, fig.width=15, fig.cap="Plotting two-way correlation matrices of the features for images 1,2 and 3.",  out.width='100%' }

par(mfrow = c(1, 3))

M <- cor(img1[3:11]) # exclude x and y in collelation matrix

c1 <- corrplot(M, method="number", title = "Image 1",mar=c(0,0,2,0))

M2 <- cor(img2[3:11])

c2 <- corrplot(M2, method="number", title = "Image 2", mar=c(0,0,2,0))

M3 <- cor(img3[3:11])

c3 <- corrplot(M3, method="number", title = "Image 3",mar=c(0,0,2,0))

```


# Modeling

## Feature Engineering 

Although many features were engineered by combining and transforming the above 
features in various combinations, we'll only present the feature
that outperformed CORR, NDAI, and SD.

### Average NDAI 

From the correlation matrix, it appeared that NDAI was the feature 
that was most correlated with the labels. Intuitively, the spacial aspect 
of this data is important. We expect "cloud" pixels and "not cloud" pixels 
to be clustered together. Or, if a label is "cloud", we would expect 
the other pixels around it to also be labeled as "cloud". Thus, we'll
construct a feature whose values for a given pixel is the average of the 
NDAI values for the 8 pixels around the pixel. If a pixel is on the edge of 
the image, the minimum value of $(x, mod(x + 4, max(x val)))$ is taken. 


```{r}
# # converted the dataframe to matrices for speed-up
# data <- as.matrix(img1[c("x", "y", "NDAI")])
# get_average_matrix <- function(index){
# 
#   this_y <- data[index, 2]
#   this_x <- data[index, 1]
#   # get all of the columns with x and y values in the range we want. This works even if the area we find is not square (because the data isn't #   square). Then find the mean of the NDAI column for this region
# 
#   avg <- mean((data[data[,1] >= this_x - 4 & data[,1] <= this_x + 4 & data[,2] >= this_y - 4 & data[,2] <= this_y + 4, ])[,3]) 
#   
#   return(avg)
# }
# 
# avg_NDAI_img1 <- lapply(1:nrow(img1), get_average_matrix)

```
```{r}
# img2_matrix <- as.matrix(img2[c("x", "y", "NDAI")])
# get_average_matrix_img2 <- function(index){
# 
#   this_y <- img2_matrix[index, 2]
#   this_x <- img2_matrix[index, 1]
# 
#   avg <- mean((img2_matrix[img2_matrix[,1] >= this_x - 4 & img2_matrix[,1] <= this_x + 4 & img2_matrix[,2] >= this_y - 4 & img2_matrix[,2] <= this_y + 4, ])[,3])
#   
#   return(avg)
# }
# 
# img3_matrix <- as.matrix(img3[c("x", "y", "NDAI")])
# get_average_matrix_img3 <- function(index){
# 
#   this_y <- img3_matrix[index, 2]
#   this_x <- img3_matrix[index, 1]
# 
#   avg <- mean((img3_matrix[img3_matrix[,1] >= this_x - 4 & img3_matrix[,1] <= this_x + 4 & img3_matrix[,2] >= this_y - 4 & img3_matrix[,2] <= this_y + 4, ])[,3])
#   
#   return(avg)
# }
# 
# avg_NDAI_img2 <- lapply(1:nrow(img2), get_average_matrix_img2)
# 
# avg_NDAI_img3 <- lapply(1:nrow(img3), get_average_matrix_img3)

```



```{r}
#img1$Avg_NDAI <- unlist(avg_NDAI_img1)
# 
#img2$Avg_NDAI <- unlist(avg_NDAI_img2)
# 
#img3$Avg_NDAI <- unlist(avg_NDAI_img3)
```

```{r}

# modify_rad <- function(DF, CF, AF, AN){
#   return((DF*CF - AN*AF)/(DF*CF + AN*AF))
# }
# 
# mult_rad <- function(DF, CF, BF, AF, AN){
#   return(DF * CF * AF * AN)
# }
# 
# mult_ndai_sd <- function(ndai, sd){
#   return(ndai * sd)
# }
# 
# # new features for image 1
# 
# img1 <- img1 %>% mutate(Sub_Feat = unlist(pmap(img1[c("DF", "CF", "AF", "AN")], function(DF, CF, AF, AN) modify_rad(DF = DF, CF = CF, AF = AF, AN = AN))))
# 
# img1 <- img1 %>% mutate(Mult_Rads = unlist(pmap(img1[c("DF", "CF", "BF", "AF", "AN")], function(DF, CF, BF, AF, AN) mult_rad(DF = DF, CF = CF, BF = BF, AF = AF, AN = AN))))
# 
# img1 <- img1 %>% mutate(Mult_AvgNDAi_SD = unlist(pmap(img1[c("Avg_NDAI", "SD")], function(Avg_NDAI, SD) mult_ndai_sd(ndai = Avg_NDAI, sd = SD))))
# 
# # repeat for image 2
# img2 <- img2 %>% mutate(Sub_Feat = unlist(pmap(img2[c("DF", "CF", "AF", "AN")], function(DF, CF, AF, AN) modify_rad(DF = DF, CF = CF, AF = AF, AN = AN))))
# 
# img2 <- img2 %>% mutate(Mult_Rads = unlist(pmap(img2[c("DF", "CF", "BF", "AF", "AN")], function(DF, CF, BF, AF, AN) mult_rad(DF = DF, CF = CF, BF = BF, AF = AF, AN = AN))))
# 
# img2 <- img2 %>% mutate(Mult_AvgNDAi_SD = unlist(pmap(img2[c("Avg_NDAI", "SD")], function(Avg_NDAI, SD) mult_ndai_sd(ndai = Avg_NDAI, sd = SD))))
# 
# # repeat for image 3
# img3 <- img3 %>% mutate(Sub_Feat = unlist(pmap(img3[c("DF", "CF", "AF", "AN")], function(DF, CF, AF, AN) modify_rad(DF = DF, CF = CF, AF = AF, AN = AN))))
# 
# img3 <- img3 %>% mutate(Mult_Rads = unlist(pmap(img3[c("DF", "CF", "BF", "AF", "AN")], function(DF, CF, BF, AF, AN) mult_rad(DF = DF, CF = CF, BF = BF, AF = AF, AN = AN))))
# 
# img3 <- img3 %>% mutate(Mult_AvgNDAi_SD = unlist(pmap(img3[c("Avg_NDAI", "SD")], function(Avg_NDAI, SD) mult_ndai_sd(ndai = Avg_NDAI, sd = SD))))

```


```{r}
# save data
# write.csv(img1, "data/img1_features.csv", row.names = FALSE)
# write.csv(img2, "data/img2_features.csv", row.names = FALSE)
# write.csv(img3, "data/img3_features.csv", row.names = FALSE)
```


### 1. Selecting The Three Best Features 

After engineering the specified feature, we'll now re-examine the 
relationship between the expert labels and these new features. This pair
plot was generated on the feature data from all 3 images combined.

```{r  fig.width = 15, fig.height=7, out.width='100%', out.height='100%', fig.cap="Pair Plot to Select Best Features"}
full_data <- rbind(img1, img2, img3)
full_data$label_final_fact <- as.factor(full_data$label_final)
ggpairs(full_data, columns = c(12, 13, 4, 5, 6), aes(color = label_final_fact, alpha = 0.5), lower = list(combo = "box", continous = "blank"), upper = list(continuous = "cor")) + theme_bw()
```


After comparing these results with the results presented above, 
we conclude that Average NDAI is the feature that is most correlated with 
the expert labels. While both NDAI and Average NDAI separte the pixels into 
cloud and not cloud distributions, the Average NDAI feature further provides 
a greater degree of separation.

### 2. Classifiers 

Logistic regression model: logistic regression is a standard and canonical choice of classification problems when the outcome is binary. In the logistic model, we assume the data points are independently and identically distributed from a model where the probability of outcome being 1 is equal to a non-linear (sigmoid) transform of some linear combinations of the covariates. Both the generalized linearity and the IID assumptions are very strong. Specifically, the independence assumption is likely to be violated in our dataset due to spatial correlation. We need to be careful when interpreting the regression coefficients. However, in this lab, our main goal is the prediction problem instead of estimating the coefficients in the specified logistic model, we focus less on the model assumption verification and evaluate the models mainly from the perspective of prediction performance. We also fit logistic regression with l1 regularization penalty to avoid over-fitting.\  

KNN: KNN is a classification model where we predict the binary outcome of a single point by referring to the values of its k nearest neighbors, where k is a hyper-parameter we need to tune. This non-parametric model does not make assumptions about independence between data points or the specific probability distributions of the data. Therefore, we are able to use it even though we observe high spatial correlations in our cloud data.\  

Random forest: random forest is an ensemble machine learning model for classification and prediction. We construct various decision trees from the bootstrapped subsample of training data and summarize the results to a predicted value of the outcome from the random forest. For each decision tree, we can train it on only a subset of all covariates to determine splits that best separate the two classes. Random forest is also a non-parametric model that makes minimal assumptions on the distribution of the data points.\  

Quadratic Discriminant Analysis (QDA): QDA is a version of Bayesian classification model which finds the linear separation in the second-order polynomial basis expansion of the covariates. It captures the lower-order linear interactions between covariates that separate the positive vs negative outcomes. The main model assumption QDA makes is that the covariates from the two classes have normal probability distributions with (possibly) different covariance matrices. As shown in our ggpair plot, not all covariates are obviously normally distributed thus it is likely the assumption is not perfectly satisfied in our data.\  

Support Vector Machines (SVM): SVM efficiently performs a non-linear classification by implicitly mapping the inputs into high-dimensional covariate spaces and then constructing a hyperplane separating the two classes of data points. It also makes the independence assumption between observations, which is likely to be violated in our dataset. Moreover, it assumes that in the true underlying data generating process, the two classes are linearly separable, which is another strong assumption in our dataset.

### 3. Fitting Models 

Before the models were fit to the data, we separated each image into a grid 
of 4 sub regions. We then randomly sampled 4 of these sub regions to use 
as a test test. The remaining 8 regions were then randomly split into training and validation sets with 4 sub regions each.

After experimenting with the inputs to the logistic regression model,
 we found that computing the logistic regression with all of the features
  and angles yielded the best results, which are presented here.

Due to computability constraints, we ran SVM on a random subsample of 50,000 
datapoints from the training data, and the model was evaluated on SOME NUMBER 
of points from the testing data. We used the default hyperparameters for 
SVM as we did not have the computational power to utilize cross validation to 
fine-tune the parameters.

We fine-tuned the hyperparameters for the KNN and Random Forest models using cross
validation. For each proposed value of the hyperparameter, the given model
was fit to the training set, before the model was evaluated on the validation set.


In the plot below, you can see that accuracy for KNN was maximized at $k=50$ nearest
neighbors.

```{r  fig.width = 15, fig.height=5, out.width='50%', out.height='50%', fig.cap="ROC Curves for Models", fig.cap="Hyperparameter tuning for KNN",fig.show='hold',fig.align='center',}

img1 <-  rasterGrob(as.raster(readPNG("figures/knn_cross_validation.png")), interpolate = FALSE)
grid.arrange(img1)

```

For the random forest classifier, we chose to fine tune the hyperparameter
that controls the number of variables that are randomly sampled as 
candidates in each split. From the plot below, we can see that a value of 2 
maximized the accuracy of the random forest classifier. 

```{r  fig.width = 15, fig.height=5, out.width='50%', out.height='50%', fig.cap="ROC Curves for Models", fig.cap="Hyperparameter tuning for Random Forest",fig.show='hold',fig.align='center',}

img1 <-  rasterGrob(as.raster(readPNG("figures/rf_cross_validation.png")), interpolate = FALSE)
grid.arrange(img1)

```


Finally, we used cross-fit to tune the hyper parameter, $\lambda$ of the regularized logistic regression.

```{r  fig.width = 15, fig.height=5,  out.width='50%', out.height='50%', fig.cap="ROC Curves for Models", fig.cap="Hyperparameter tuning for Logistic Regression",fig.show='hold',fig.align='center',}

img1 <-  rasterGrob(as.raster(readPNG("figures/logit_cross_fit.png")), interpolate = FALSE)
grid.arrange(img1)

```

As you can see, the mean squared error is minimized when $\lambda = 0.00019$. Thus,
we conclude that regularization does not benefit this model. 

The results for the models on the test set are presented below.

```{r  fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="ROC Curves for Models", fig.cap="caption",fig.show='hold',fig.align='center'}
par(mfrow = c(1, 2))
img1 <-  rasterGrob(as.raster(readPNG("figures/roc_curves.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("figures/pr_curves.png")), interpolate = FALSE)
grid.arrange(img1, img2, ncol = 2)


```

The plots of the ROC and PR curves demonstrate that Logistic Regression performs 
the best of all of the classifiers implemented. However, as you can see in the 
table below, the results of the models are all comparable, with only minor differences 
in accuracy between them. 

```{r}
library(flextable)
library(huxtable)
library(formatR)
results <- read.csv("results/error_metrics.csv")
results <- results[-c(1)] %>% relocate(method)

ft <- flextable(results) %>% 
  autofit() %>%
  #align(align = "center", part = "all") %>% 
  line_spacing(space = .5, part = "all") %>% 
  padding(padding = 1, part = "header") 

ft
```
Looking at these metrics, we can see that the models have high accuracy with 
balanced proportions of false positives and false negatives. Logistic regression
maintains the highest accuracy and precision, but the KNN model has similar accuracy and 
higher recall. Since in this application we are more concerned with false positives,
or ground being labeled as clouds, logistic regression is the preferred model.

### 4. Random Forest Convergence and Feature Importance

```{r  fig.width = 15, fig.height=5, out.width='50%', out.height='50%', fig.cap="ROC Curves for Models", fig.cap="Convergence of the Random Forest model",fig.show='hold',fig.align='center'}
img1 <-  rasterGrob(as.raster(readPNG("Error/convergence.png")), interpolate = FALSE)

grid.arrange(img1, ncol = 1)
```
Here, we are assessing the convergence of our Random Forest model using the Out of Bag (OOB) error. As shown on the figure, the OOB error decreases as more trees are added to the model. Convergence seems to be achieved after adding approximately 250 trees. 

```{r  fig.width = 15, fig.height=5, out.width='50%', out.height='50%', fig.cap="ROC Curves for Models", fig.cap=" Ploting the Feature Importance of the Random Forest model",fig.show='hold',fig.align='center'}
img1 <-  rasterGrob(as.raster(readPNG("figures/rf_feature_importance.png")), interpolate = FALSE)

grid.arrange(img1, ncol = 1)
```

Next, we show the feature importance of the Random Forest Model through the Mean Decrease Accuracy and Mean Decrease Gini. The Mean Decrease Accuracy shows how much accuracy the model loses by removing a feature and the Mean Decrease Gini expresses the average gain of homogeneity by splits of a given variable. The higher these metrics for each feature, the more important the feature is. As shown on the figure, the most important feature seems to be the Average NDAI.  

### 5. Post-Hoc EDA 

Here, we present visual and quantitative analysis of the misclassified data for the logistic regression model. 

```{r fig.width = 8, fig.height=5, out.width='60%', out.height='80%',fig.cap="Comparing missclassified test data to ground truth ",fig.show='hold',fig.align='center'}

#loading test prediction and full data
test_pred <- read.csv("results/logit_predictions.csv")
full_data <- read.csv("data/full_data.csv")

#filter misclassified pixels
img_miss <- test_pred %>%
  filter(logit_pred_binary != X.1)

#find test data
full_data$pixel_number <- 1:nrow(full_data)
test_data <- full_data %>%
  filter(pixel_number %in% test_pred$X)

test_data_img2 <- test_data %>% 
  filter(image == "image2") 

test_data_img3 <- test_data %>% 
  filter(image == "image3")

# find misclassified test data
test_miss <- full_data %>% 
  filter(pixel_number %in% img_miss$X) 

test_miss_img2 <- test_miss %>% 
  filter(image == "image2") 

test_miss_img3 <- test_miss %>% 
  filter(image == "image3")

gt_heatmap <- function(index, img, miss_or_not){
  return(ggplot(img, aes(x = x,y = y, fill = label_fact)) + geom_tile() + labs(title = paste(c("Misclassification for Test Data - Image ","Ground Truth for Test Data - Image  ")[miss_or_not], index)) +
  scale_fill_brewer(labels = c("Not Cloud", "Cloud"), palette = "Blues", direction = -1, name = "") + theme(panel.background =     element_blank(),plot.background = element_blank(), axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()))
}

test_miss_img2$label_fact <- as.factor(test_miss_img2$label_final)
test_data_img2$label_fact <- as.factor(test_data_img2$label_final)
test_miss_img3$label_fact <- as.factor(test_miss_img3$label_final)
test_data_img3$label_fact <- as.factor(test_data_img3$label_final)

map1 <- gt_heatmap(2, test_miss_img2, 1)
map2 <- gt_heatmap(2, test_data_img2, 2)
map3 <- gt_heatmap(3, test_miss_img3, 1)
map4 <- gt_heatmap(3, test_data_img3, 2)

ggarrange(map1, map2, map3, map4, common.legend = TRUE, ncol = 2, nrow = 2)


```

From the plot, and especially for image 2, we notice that the model has trouble classifying pixels that are on the borders between "cloud" and "not cloud" regions. Most of the misclassified pixels seem to be "not cloud" regions. When comparing these plots to the expert labels, we notice that the misclassified labels  for "not cloud" regions are next to unlabeled regions.\  

We also investigated the distribution of each feature for misclassified and correctly classified data.

```{r distribution, fig.width = 12, fig.height=6, out.width='100%', out.height='100%', fig.cap="Plotting distributions of test data for all features " }

#loading test prediction and full data
test_pred <- read.csv("results/logit_predictions.csv")
full_data <- read.csv("data/full_data.csv")

#filter misclassified pixels
img_miss <- test_pred %>%
  filter(logit_pred_binary != X.1)

#find test data
full_data$pixel_number <- 1:nrow(full_data)
test_data <- full_data %>%
  filter(pixel_number %in% test_pred$X)

# find misclassified test data
test_miss <- full_data %>% 
  filter(pixel_number %in% img_miss$X) 

# find correctly classified test data
test_correct <- test_data %>% 
  subset(., !(pixel_number %in% img_miss$X)) 

plot_distribution <- function(index, df, feats){
  return(ggplot(df, aes(x = {{feats}}, fill = factor(label_final))) + geom_density(alpha = 0.4)+ labs(title = c("Misclassified data","Correctly classified data")[index]) + scale_fill_manual(labels=c("Not Cloud", "Cloud"), values = c("blue", "red"), name = "") + theme_bw())
}

ndai_miss_dist <- plot_distribution(index=1, df=test_miss, feats= NDAI)
ndai_corr_dist <- plot_distribution(index=2, df=test_correct, feats= NDAI)

sd_miss_dist <- plot_distribution(index=1, df=test_miss, feats= SD)
sd_corr_dist <- plot_distribution(index=2, df=test_correct, feats= SD)

corr_miss_dist <- plot_distribution(index=1, df=test_miss, feats= CORR)
corr_corr_dist <- plot_distribution(index=2, df=test_correct, feats= CORR)

df_miss_dist <- plot_distribution(index=1, df=test_miss, feats= DF)
df_corr_dist <- plot_distribution(index=2, df=test_correct, feats= DF)

cf_miss_dist <- plot_distribution(index=1, df=test_miss, feats= CF)
cf_corr_dist <- plot_distribution(index=2, df=test_correct, feats= CF)

bf_miss_dist <- plot_distribution(index=1, df=test_miss, feats= BF)
bf_corr_dist <- plot_distribution(index=2, df=test_correct, feats= BF)

af_miss_dist <- plot_distribution(index=1, df=test_miss, feats= AF)
af_corr_dist <- plot_distribution(index=2, df=test_correct, feats= AF)

an_miss_dist <- plot_distribution(index=1, df=test_miss, feats= AN)
an_corr_dist <- plot_distribution(index=2, df=test_correct, feats= AN)

plot <- ggarrange(ndai_miss_dist, ndai_corr_dist, sd_miss_dist, sd_corr_dist,
          corr_miss_dist, corr_corr_dist, df_miss_dist, df_corr_dist,
          cf_miss_dist, cf_corr_dist, bf_miss_dist, bf_corr_dist,
          af_miss_dist, af_corr_dist, an_miss_dist, an_corr_dist,
          ncol = 4, nrow = 4, common.legend = TRUE)

```

By looking at the plot, we notice that the feature range and distribution for the misclassified data seem to be unusual; They seem to be opposite to the general trend of the correctly classified data.

Next, we investigate the predictions quantitatively. 

```{r, fig.width=17, fig.height=5, fig.cap="Plotting two-way correlation matrices of the features of predictions.",  out.width='100%' }

par(mfrow = c(1, 2))

M <- cor(test_miss[4:13]) # exclude x and y in collelation matrix

c1 <- corrplot(M, method="number", title = "Misclassified data",mar=c(0,0,2,0))

M2 <- cor(test_correct[4:13])

c2 <- corrplot(M2, method="number", title = "Correctly classified data", mar=c(0,0,2,0))


```

The correlation metrics show that the final label for the misclassiied data are negatively correlated with NDAI, SD, CORR, Avg_NDAI, while they are positively correlated to the same features for the correctly classified data. Moreover, the final label is positively correlated to the radiance angles, while they are negatively correlated for the correctly classified data. 

### 6. Implication of Results in an Unsupervised Setting 


# Academic honesty statement


# Collaborators

We'd like to thank our GSI, Theo Saarinen, for his support 
and direction on feature engineering. 


# Bibliography

Ahima S. Rexford (2020) Global warming threatens human thermoregulation and survival, J Clin Invest, 130(2):559-561, DOI: 10.1172/JCI135006. 

Tao Shi, Bin Yu, Eugene E Clothiaux & Amy J Braverman (2008) Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies, Journal of the American Statistical Association, 103:482, 584-593, DOI: 10.1198/016214507000001283.

