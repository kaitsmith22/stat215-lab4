---
title: "Lab 4 - Cloud Data, Stat 215A, Fall 2022"
author: "Sizhu Lu, Kaitlin Smith, Ghafar Yerima"
date: "`r format(Sys.time(), '%B %d, %Y')`"
header-includes:
   - \usepackage{float}
output: 
  pdf_document:
    number_sections: true
---

```{r setup, echo = FALSE, message=FALSE, warning=FALSE}
# load in useful packages
library(tidyverse)
library(stringr)
library(ggplot2)
library(ggpubr)
library(dplyr)
library(corrplot)
library(purrr)
library(gridExtra)
library(knitr)
library("GGally")
library(png)
library(grid)

# set default knitr chunks
knitr::opts_chunk$set(
  echo = FALSE,  # don't print the code chunk
  warning = FALSE,  # don't print warnings
  message = FALSE,  # don't print messages
  fig.width = 6,  # set default width of figures
  fig.height = 4,  # set default height of figures
  fig.align = "center",  # always align figure in center
  fig.pos = "H",  # always plot figure at the exact location of the code chunk
  cache = FALSE)  # don't cache results

```

```{r load-data, echo = FALSE, message = FALSE, warning = FALSE, cache = TRUE}
feature_names <- c("x", "y", "label", "NDAI", "SD","CORR", "DF", "CF", "BF", "AF", "AN")
img1 <- read.csv("data/img1_features.csv")
img1$label_fact <- as.factor(img1$label) # make label a factor for plotting, but needs to be a number for correlation
img1 <- img1[-c(16)] # remove extra feature I accidentally only generated for this one image
img2 <- read.csv("data/img2_features.csv")
img2$label_fact <- as.factor(img2$label)
img3 <- read.csv("data/img3_features.csv")
img3$label_fact <- as.factor(img3$label)

img1$label_final <- replace(img1$label, img1$label == 0, 1)
img2$label_final <- replace(img2$label, img2$label == 0, 1)
img3$label_final <- replace(img3$label, img3$label == 0, 1)

```

# Introduction

  Global warming is characterized by the rapid increase in Earth's surface temperature over the past century; It is due primarily to human activities including fossil fuels burning, which increases greenhouse gases (Rexford 2020). The Earth surface temperature has increased by nearly 1 degree Celsius between 1905 and 2005, and is predicted to increase between 2-6 degree Celsius by the end of the 21st century (Rexford 2020). Models have predicted that the strongest dependencies between greenhouse gases level and earth's surface temperature will occur at the Arctic (Shi et al. 2008). As the Arctic warms faster the rest of the globe, alterations in the properties and distribution of ice-and snow-covered surfaces, atmospheric vapor, and clouds can lead to further warming, and more susceptibility to greenhouse gases levels (Shi et al. 2008). To understand these intricate relationships, a study of the cloud coverage at the Arctic is necessary as clouds play a crucial role in tuning the sensitivity of the Arctic to the rise of surface air temperatures (Shi et al. 2008).\  
  
  However, studying Arctic clouds is challenging because liquid and ice water cloud particles have similar physical properties to ice and snow surfaces particles. Thus, distinguishing Arctic clouds from water or ice using their electromagnetic radiation is challenging. Thanks to a National Aeronautics, and Space Administration (NASA) effort, a Multiangle Imaging SpectroRadiometer (MISR) was launched on the Terra satellite in 1999. This radiometer produces novel radiation measurements from nine view angles. The nine view zenith angles of the cameras are 70.5" (Df), 60.0" (Cf), 45.6" (Bf), and 26.1" (Af) in the forward direction; 0.0" (An) in the nadir direction and 26.1" (Aa), 45.6" (Ba), 60.0" (Ca), and 70.5" (Da) in the aft direction. Thus, the MISR produces a large amount of data thanks to its 360 km-wide swath coverage on the Earth's surface that extends from the Arctic down to Antarctica in approximately 45 minutes.\  

  Although the MISR produces a high resolution data, its operational cloud detection system was designed before its launch and proved to not be effective for detecting clouds over bright surfaces in polar regions. Solving the polar cloud detection problem requires efficient statistical models, capable of handling the massive data set while incorporating scientific and operational considerations.\  
  
  In this paper, we explored a MISR data of polar regions and implemented various polar cloud detection algorithms based on expert labels. The algorithms implemented include logistic regression (w/ & w/o regularization), KNN, SVM, RF, and a Bayesian method. Ultimately, we aimed to design algorithms that will perform well on data sets lacking any expert labels. 
  

# Data

  Our data comprises 3 images which represent pictures taken from the MISR. Each image text file comprises 11 columns. The first two columns represent the y and x coordinates of the pixel, and the third column represent the expert label (+1= cloud, -1= not cloud, 0= unlabeled). The fourth, fifth, and sixth columns were features engineered by Shi et. al. They are: the Normalized Difference Angular Index (NDAI) which characterizes the variations in a scene with the variations in the MISR view direction, the standard deviation (SD) of MISR nadir camera pixel values across a scene, and the correlation (CORR) of MISR images of the same scene from different MISR viewing directions (). Columns 7 to 11 represent the radiance angles DF, CF, BF, AF, and AN from the MISR.
  
## Data Collection

  The data used in our study is a subset of the MISR data set from Shi et. al. It was collected from 10 MISR orbits of path 26 over the Arctic, Northern Greenland, and Baffin Bay (Shi et al. 2008). As the MISR collects data from 233 geographically distinct but overlapping paths on a repeat cycle over 16 days, the 10 orbits spanned approximately 144 days from April 28 to September 19, 2002. Path 26 was chosen for the richness of its surface features and their relative stability over the study period. From the 6 data units per orbits included in their study, 3 were excluded as they were open water after the ice melted over the summer.  

## Data Exploration

### 1. Expert labels 

First, we'll look at the the expert labels for the presece of clouds:
<br> 
<br> 

```{r expert labels, fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="Expert Cloud Labels" }
heatmap_ggplot <- function(index, img){
  return(ggplot(img, aes(x = x,y = y, fill = img[,index])) + geom_tile() + labs(title = colnames(img)[index]) + theme(panel.background = element_blank(),plot.background = element_blank(), axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()))
}

gt_heatmap <- function(index, img){
  return(ggplot(img, aes(x = x,y = y, fill = label_fact)) + geom_tile() + labs(title = paste("Ground Truth for Image ", index)) +
  scale_fill_brewer(labels = c("Not Cloud", "Unlabeled", "Cloud"), palette = "Blues", direction = -1, name = "") + theme(panel.background =     element_blank(),plot.background = element_blank(), axis.text.x=element_blank(),
      axis.ticks.x=element_blank(),
      axis.text.y=element_blank(),
      axis.ticks.y=element_blank()))
}

map1 <- gt_heatmap(1, img1)
map2 <- gt_heatmap(2, img2)
map3 <- gt_heatmap(3, img3)

ggarrange(plotlist = c(list(map1), list(map2), list(map3)), ncol = 3, common.legend = TRUE)
``` 

  Looking at the initial expert labels for image 1 and 3, we notice that the clouds occupy roughly the third of the map. However, the proportion of labeled clouds on image 2 is substantially larger. The different radiance angles seem to be correlated for the most part. They show approximately the same distribution and intensity of pixels. 
  
### Rational for Converting Unlabeled pixels to "Cloud" Labels 

  Next, we explored the distribution of the cloud labels. It appeared that the cloud labels is contained in the labels that were unlabeled. For this reason, we decided to consider the unlabeled pixels as cloudy pixels.

```{r NDAI distribution, fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="Plotting distributions of cloud expert labels vs NDAI" }
# TODO: CHANGE COLORS

ndai_label1 <- ggplot(img1, aes(x = NDAI, fill = factor(label))) + geom_density(alpha = 0.4)+ labs(title = "Image1") + scale_fill_manual(labels=c("Not Cloud", "Unlabeled", "Cloud"), values = c("blue", "red", "green"), name = "") + theme_bw()

ndai_label2 <- ggplot(img2, aes(x = NDAI, fill = factor(label))) + geom_density(alpha = 0.4) + labs(title = "Image2") + scale_fill_manual(labels=c("Not Cloud", "Unlabeled", "Cloud"), values = c("blue", "red", "green"), name = "")+ theme_bw()

ndai_label3 <- ggplot(img3, aes(x = NDAI, fill = factor(label))) + geom_density(alpha = 0.4) + labs(title = "Image3") +scale_fill_manual(labels=c("Not Cloud", "Unlabeled", "Cloud"), values = c("blue", "red", "green"), name = "")+ theme_bw()

ggarrange(ndai_label1, ndai_label2, ndai_label3, ncol = 3, common.legend = TRUE)
```

Thus, we added "label_final" feature that converts all of the unlabeled pixels to cloud labels. Following this conversion, we notice that our images now contain more cloudy pixels than non-cloudy ones.

```{r}
img1$label_final <- replace(img1$label, img1$label == 0, 1)
img2$label_final <- replace(img2$label, img2$label == 0, 1)
img3$label_final <- replace(img3$label, img3$label == 0, 1)

```

### 2. 

First, we'll look at relationships between the radiances of different angles visually.

```{r image1, fig.width = 15, fig.height=10, fig.asp= 0.6, out.width='100%', out.height='100%', fig.cap="Plotting angles on maps" }
heatmap_ggplot <- function(index, img){
  return(ggplot(img, aes(x = x,y = y, fill = img[,index])) + geom_tile() + labs(title = colnames(img)[index]) + theme(panel.background =     element_blank(),plot.background = element_blank()))
}

ang1 <- lapply(7:11, heatmap_ggplot, img1)
ang2 <- lapply(7:11, heatmap_ggplot, img2)
ang3 <- lapply(7:11, heatmap_ggplot, img3)

angle1 <- ggarrange(plotlist = ang1, common.legend = TRUE, ncol = 5)
angle1 <- annotate_figure(angle1, top = text_grob("Angles for Image 1"))

angle2 <- ggarrange(plotlist = ang2, common.legend = TRUE, ncol = 5)
angle2 <- annotate_figure(angle2, top = text_grob("Angles for Image 2"))

angle3 <- ggarrange(plotlist = ang3, common.legend = TRUE, ncol = 5)
angle3 <- annotate_figure(angle3, top = text_grob("Angles for Image 3"))

ggarrange(plotlist = c(list(angle1), list(angle2), list(angle3)), nrow = 3)
``` 

Visually, it appears like the DF and AN bands are inverses of one another. Particularly
 for Image 2, When the DF band registers higher values, the AN band registers lower quantities. Comparing these plots to the expert labels, it appears like clouds 
 correspond to high values of DF and CF, which is particularly apparent in image 2.
 
 Now, looking at the CORR, NDAI, and SD features, we see stronger relationships
 between the presence of clouds and these features: 
 
```{r features, fig.width = 15, fig.height=10, fig.asp= 0.6, out.width='100%', out.height='100%', fig.cap="Plotting features on maps", fig.align="center"}
ang1 <- lapply(4:6, heatmap_ggplot, img1)
ang2 <- lapply(4:6, heatmap_ggplot, img2)
ang3 <- lapply(4:6, heatmap_ggplot, img3)

angle1 <- ggarrange(plotlist = ang1, common.legend = TRUE, ncol = 5)
angle1 <- annotate_figure(angle1, top = text_grob("Features for Image 1"))

angle2 <- ggarrange(plotlist = ang2, common.legend = TRUE, ncol = 5)
angle2 <- annotate_figure(angle2, top = text_grob("Features for Image 2"))

angle3 <- ggarrange(plotlist = ang3, common.legend = TRUE, ncol = 5)
angle3 <- annotate_figure(angle3, top = text_grob("Features for Image 3"))

ggarrange(plotlist = c(list(angle1), list(angle2), list(angle3)), nrow = 3)

```
 
It appears as though high values of NDAI indicate the presence of clouds. However,
the highest values of NDAI correspond to the regions that the expert failed to label.
More subtlety, you can see that higher values of CORR also indicate the presence of 
clouds. 

  Our next step was exploring the features quantitatively. Thus, we investigated the correlation of our features to the labels for each image. As shown below, it appears that NDAI has the strongest correlation with the labels. For image1 and image2, it appears that the angles of radiance also have a negative correlation. However, image3 does not have this relationship with the labels. In addition, it appears that NDAI and SD have a positive correlation.
  
  
```{r, fig.width=15, fig.cap="Plotting two-way correlation matrices of the features for images 1,2 and 3.",  out.width='100%' }

par(mfrow = c(1, 3))

M <- cor(img1[3:11]) # exclude x and y in collelation matrix

c1 <- corrplot(M, method="number", title = "Image 1",mar=c(0,0,2,0))

M2 <- cor(img2[3:11])

c2 <- corrplot(M2, method="number", title = "Image 2", mar=c(0,0,2,0))

M3 <- cor(img3[3:11])

c3 <- corrplot(M3, method="number", title = "Image 3",mar=c(0,0,2,0))

```


# Modeling

## Feature Engineering 

Although many features were engineered by combining and transforming the above 
features in various combinations, we'll only present the feature
that outperformed CORR, NDAI, and SD.

### Average NDAI 

From the correlation matrix, it appeared that NDAI was the feature 
that was most correlated with the labels. Intuitively, the spacial aspect 
of this data is important. We expect "cloud" pixels and "not cloud" pixels 
to be clustered together. Or, if a label is "cloud", we would expect 
the other pixels around it to also be labeled as "cloud". Thus, we'll
construct a feature whose values for a given pixel is the average of the 
NDAI values for the 8 pixels around the pixel. If a pixel is on the edge of 
the image, the minimum value of $(x, mod(x + 4, max(x val)))$ is taken. 


```{r}
# # converted the dataframe to matrices for speed-up
# data <- as.matrix(img1[c("x", "y", "NDAI")])
# get_average_matrix <- function(index){
# 
#   this_y <- data[index, 2]
#   this_x <- data[index, 1]
#   # get all of the columns with x and y values in the range we want. This works even if the area we find is not square (because the data isn't #   square). Then find the mean of the NDAI column for this region
# 
#   avg <- mean((data[data[,1] >= this_x - 4 & data[,1] <= this_x + 4 & data[,2] >= this_y - 4 & data[,2] <= this_y + 4, ])[,3]) 
#   
#   return(avg)
# }
# 
# avg_NDAI_img1 <- lapply(1:nrow(img1), get_average_matrix)

```
```{r}
# img2_matrix <- as.matrix(img2[c("x", "y", "NDAI")])
# get_average_matrix_img2 <- function(index){
# 
#   this_y <- img2_matrix[index, 2]
#   this_x <- img2_matrix[index, 1]
# 
#   avg <- mean((img2_matrix[img2_matrix[,1] >= this_x - 4 & img2_matrix[,1] <= this_x + 4 & img2_matrix[,2] >= this_y - 4 & img2_matrix[,2] <= this_y + 4, ])[,3])
#   
#   return(avg)
# }
# 
# img3_matrix <- as.matrix(img3[c("x", "y", "NDAI")])
# get_average_matrix_img3 <- function(index){
# 
#   this_y <- img3_matrix[index, 2]
#   this_x <- img3_matrix[index, 1]
# 
#   avg <- mean((img3_matrix[img3_matrix[,1] >= this_x - 4 & img3_matrix[,1] <= this_x + 4 & img3_matrix[,2] >= this_y - 4 & img3_matrix[,2] <= this_y + 4, ])[,3])
#   
#   return(avg)
# }
# 
# avg_NDAI_img2 <- lapply(1:nrow(img2), get_average_matrix_img2)
# 
# avg_NDAI_img3 <- lapply(1:nrow(img3), get_average_matrix_img3)

```



```{r}
#img1$Avg_NDAI <- unlist(avg_NDAI_img1)
# 
#img2$Avg_NDAI <- unlist(avg_NDAI_img2)
# 
#img3$Avg_NDAI <- unlist(avg_NDAI_img3)
```

```{r}

# modify_rad <- function(DF, CF, AF, AN){
#   return((DF*CF - AN*AF)/(DF*CF + AN*AF))
# }
# 
# mult_rad <- function(DF, CF, BF, AF, AN){
#   return(DF * CF * AF * AN)
# }
# 
# mult_ndai_sd <- function(ndai, sd){
#   return(ndai * sd)
# }
# 
# # new features for image 1
# 
# img1 <- img1 %>% mutate(Sub_Feat = unlist(pmap(img1[c("DF", "CF", "AF", "AN")], function(DF, CF, AF, AN) modify_rad(DF = DF, CF = CF, AF = AF, AN = AN))))
# 
# img1 <- img1 %>% mutate(Mult_Rads = unlist(pmap(img1[c("DF", "CF", "BF", "AF", "AN")], function(DF, CF, BF, AF, AN) mult_rad(DF = DF, CF = CF, BF = BF, AF = AF, AN = AN))))
# 
# img1 <- img1 %>% mutate(Mult_AvgNDAi_SD = unlist(pmap(img1[c("Avg_NDAI", "SD")], function(Avg_NDAI, SD) mult_ndai_sd(ndai = Avg_NDAI, sd = SD))))
# 
# # repeat for image 2
# img2 <- img2 %>% mutate(Sub_Feat = unlist(pmap(img2[c("DF", "CF", "AF", "AN")], function(DF, CF, AF, AN) modify_rad(DF = DF, CF = CF, AF = AF, AN = AN))))
# 
# img2 <- img2 %>% mutate(Mult_Rads = unlist(pmap(img2[c("DF", "CF", "BF", "AF", "AN")], function(DF, CF, BF, AF, AN) mult_rad(DF = DF, CF = CF, BF = BF, AF = AF, AN = AN))))
# 
# img2 <- img2 %>% mutate(Mult_AvgNDAi_SD = unlist(pmap(img2[c("Avg_NDAI", "SD")], function(Avg_NDAI, SD) mult_ndai_sd(ndai = Avg_NDAI, sd = SD))))
# 
# # repeat for image 3
# img3 <- img3 %>% mutate(Sub_Feat = unlist(pmap(img3[c("DF", "CF", "AF", "AN")], function(DF, CF, AF, AN) modify_rad(DF = DF, CF = CF, AF = AF, AN = AN))))
# 
# img3 <- img3 %>% mutate(Mult_Rads = unlist(pmap(img3[c("DF", "CF", "BF", "AF", "AN")], function(DF, CF, BF, AF, AN) mult_rad(DF = DF, CF = CF, BF = BF, AF = AF, AN = AN))))
# 
# img3 <- img3 %>% mutate(Mult_AvgNDAi_SD = unlist(pmap(img3[c("Avg_NDAI", "SD")], function(Avg_NDAI, SD) mult_ndai_sd(ndai = Avg_NDAI, sd = SD))))

```


```{r}
# save data
# write.csv(img1, "data/img1_features.csv", row.names = FALSE)
# write.csv(img2, "data/img2_features.csv", row.names = FALSE)
# write.csv(img3, "data/img3_features.csv", row.names = FALSE)
```


### 1. Selecting The Three Best Features 

After engineering the specified feature, we'll now re-examine the 
relationship between the expert labels and these new features. This pair
plot was generated on the feature data from all 3 images combined.

```{r  fig.width = 15, fig.height=7, out.width='100%', out.height='100%', fig.cap="Pair Plot to Select Best Features"}
full_data <- rbind(img1, img2, img3)
full_data$label_final_fact <- as.factor(full_data$label_final)
ggpairs(full_data, columns = c(12, 13, 4, 5, 6), aes(color = label_final_fact, alpha = 0.5), lower = list(combo = "box", continous = "blank"), upper = list(continuous = "cor")) + theme_bw()
```


After comparing these results with the results presented above, 
we conclude that Average NDAI is the feature that is most correlated with 
the expert labels. While both NDAI and Average NDAI separte the pixels into 
cloud and not cloud distributions, the Average NDAI feature further provides 
a greater degree of separation.

### 2. Classifiers 

### 3. Fitting Models 

Before the models were fit to the data, we separated each image into a grid 
of 4 sub regions. We then randomly sampled 4 of these sub regions to use 
as a test test. The remaining 8 regions were then randomly split into training and validation sets with 4 sub regions each.

After experimenting with the inputs to the logistic regression model,
 we found that computing the logistic regression with all of the features
  and angles yielded the best results, which are presented here.

Due to computability constraints, we ran SVM on a random subsample of 50,000 
datapoints from the training data, and the model was evaluated on SOME NUMBER 
of points from the testing data. We used the default hyperparameters for 
SVM as we did not have the computational power to utilize cross validation to 
fine-tune the parameters.

We fine-tuned the hyperparameters for the KNN and Random Forest models using cross
validation. For each proposed value of the hyperparameter, the given model
was fit to the training set, before the model was evaluated on the validation set.


In the plot below, you can see that accuracy for KNN was maximized at $k=50$ nearest
neighbors.

```{r  fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="ROC Curves for Models", fig.cap="Hyperparameter tuning for KNN",fig.show='hold',fig.align='center',}

img1 <-  rasterGrob(as.raster(readPNG("figures/knn_cross_validation.png")), interpolate = FALSE)
grid.arrange(img1)

```

For the random forest classifier, we chose to fine tune the hyperparameter
that controls the number of variables that are randomly sampled as 
candidates in each split. From the plot below, we can see that a value of 2 
maximized the accuracy of the random forest classifier. 

```{r  fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="ROC Curves for Models", fig.cap="Hyperparameter tuning for Random Forest",fig.show='hold',fig.align='center',}

img1 <-  rasterGrob(as.raster(readPNG("figures/rf_cross_validation.png")), interpolate = FALSE)
grid.arrange(img1)

```


Finally, we used cross-fit to tune the hyper parameter, $\lambda$ of the regularized logistic regression.

```{r  fig.width = 15, fig.height=5,  out.width='100%', out.height='100%', fig.cap="ROC Curves for Models", fig.cap="Hyperparameter tuning for Logistic Regression",fig.show='hold',fig.align='center',}

img1 <-  rasterGrob(as.raster(readPNG("figures/logit_cross_fit.png")), interpolate = FALSE)
grid.arrange(img1)

```

As you can see, the mean squared error is minimized when $\lambda = 0.00019$. Thus,
we conclude that regularization does not benefit this model. 

The results for the models on the test set are presented below.

```{r  fig.width = 15, fig.height=5, out.width='100%', out.height='100%', fig.cap="ROC Curves for Models", fig.cap="caption",fig.show='hold',fig.align='center'}
par(mfrow = c(1, 2))
img1 <-  rasterGrob(as.raster(readPNG("figures/roc_curves.png")), interpolate = FALSE)
img2 <- rasterGrob(as.raster(readPNG("figures/pr_curves.png")), interpolate = FALSE)
grid.arrange(img1, img2, ncol = 2)


```

The plots of the ROC and PR curves demonstrate that Logistic Regression performs 
the best of all of the classifiers implemented. However, as you can see in the 
table below, the results of the models are all comparable, with only minor differences 
in accuracy between them. 

```{r}
library(flextable)
library(huxtable)
library(formatR)
results <- read.csv("results/error_metrics.csv")
results <- results[-c(1)] %>% relocate(method)

ft <- flextable(results) %>% 
  autofit() %>%
  #align(align = "center", part = "all") %>% 
  line_spacing(space = .5, part = "all") %>% 
  padding(padding = 1, part = "header") 

ft
```
Looking at these metrics, we can see that the models have high accuracy with 
balanced proportions of false positives and false negatives. Logistic regression
maintains the highest accuracy and precision, but the KNN model has similar accuracy and 
higher recall. Since in this application we are more concerned with false positives,
or ground being labeled as clouds, logistic regression is the preferred model.

### 4. Random Forest Convergence 

### 5. Post-Hoc EDA 

### 6. Implication of Results in an Unsupervised Setting 


# Academic honesty statement


# Collaborators

We'd like to thank our GSI, Theo Saarinen, for his support 
and direction on feature engineering. 


# Bibliography

Ahima S. Rexford (2020) Global warming threatens human thermoregulation and survival, J Clin Invest, 130(2):559-561, DOI: 10.1172/JCI135006. 

Tao Shi, Bin Yu, Eugene E Clothiaux & Amy J Braverman (2008) Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies, Journal of the American Statistical Association, 103:482, 584-593, DOI: 10.1198/016214507000001283.

